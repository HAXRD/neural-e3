import pdb
import os, pdb, time, torch, gc, random, numpy, scipy, datetime, torchvision
import sys, logging, traceback, argparse
import torch.optim as optim
import torch.nn as nn
import matplotlib.pyplot as plt
import environment, agents, plotting, models, utils, importlib
from utils import Tensorboard
import search

parser = argparse.ArgumentParser()
parser.add_argument('-env', type=str, default='mountaincar')
parser.add_argument('-system', type=str, default='local')
parser.add_argument('-split', type=str, default='test')
parser.add_argument('-method', type=str, default='dqn')
parser.add_argument('-rscale', type=float, default=1.0)
parser.add_argument('-phi', type=str, default='none')
parser.add_argument('-grad_clip', type=float, default=10.0)
parser.add_argument('-update_target_freq', type=int, default=10)
parser.add_argument('-seed', type=int, default=12345)
parser.add_argument('-n_action_repeat', type=int, default=4)
parser.add_argument('-n_input_frames', type=int, default=1)
parser.add_argument('-n_feature_maps', type=int, default=64)
parser.add_argument('-n_hidden', type=int, default=256)
parser.add_argument('-edim', type=int, default=2)
parser.add_argument('-spherenorm', type=int, default=0)
parser.add_argument('-learn_radius', type=int, default=1)
parser.add_argument('-p_dropout', type=float, default=0.0)
parser.add_argument('-n_ensemble', type=int, default=8)
parser.add_argument('-uest', type=str, default='ensemble-max-gpu', help='uncertainty estimation')
parser.add_argument('-u_quantile', type=float, default=-1)
parser.add_argument('-eps', type=float, default=0.0005)
parser.add_argument('-alpha', type=float, default=0.0001)
parser.add_argument('-beta', type=float, default=1.0)
parser.add_argument('-loss', type=str, default='fwd')
parser.add_argument('-phi_decode', type=int, default=0)
parser.add_argument('-phi_layer_size', type=int, default=512)
parser.add_argument('-batch_size', type=int, default=64)
parser.add_argument('-learning_rate', type=float, default=0.0005)
parser.add_argument('-n_model_updates', type=int, default=500)
parser.add_argument('-exploration_policy', type=str, default='particle')
parser.add_argument('-n_exploration_episodes', type=int, default=10)
parser.add_argument('-gamma', type=float, default=0.99)
parser.add_argument('-pos_reward_prob', type=float, default=0.25)
parser.add_argument('-debug', type=int, default=0)
parser.add_argument('-cuda', type=int, default=1)
parser.add_argument('-max_exploration_steps', type=int, default=200)
parser.add_argument('-zeroact', type=int, default=0)
parser.add_argument('-T', type=int, default=20)
parser.add_argument('-a_combine', type=str, default='mult')
parser.add_argument('-freeze_ends', type=int, default=0)
parser.add_argument('-diag_radius', type=float, default=5.0)
parser.add_argument('-warmstart', type=str, default='')
parser.add_argument('-test', type=int, default=1)
parser.add_argument('-results_dir', type=str, default='/exploit3/')
config = parser.parse_args()

config.n_input_channels = 2
config.n_actions = 3
config.height = 1
config.width = 1
config.image_subsample = 1
config.phi_layer_size = 2
config.n_action_repeat = 1
config.learn_phi = 0
config.spherenorm = 0
config.learn_radius = 0
config.rmin = -1.0
config.rmax = 0.0

env = environment.EnvironmentWrapper(config)

if config.system == 'local' or config.system == 'gcr':
    config.results_dir = '/nycml/mihenaff/results/' + config.results_dir
    config.checkpoint_file = '/nycml/mihenaff/results/v14/env=mountaincar-explore=particle-loss=fwd-bsize=64-negsamp=63-nfeature=64-nhidden=64-edim=2-inputframes=1-T=20-lrt=0.0001-dropout=0.0-zeroact=0-acomb=mult-snorm=0-learnradius=1-uest=ensemble-var-gpu-uquant=0.9-ensemble=8-nexplore=10-nupdate=100/agent.step8.pth'    
elif config.system == 'philly':
    config.results_dir = os.getenv('PT_OUTPUT_DIR') + config.results_dir
    config.checkpoint_file = os.getenv('PT_DATA_DIR') + '/replay_memory.pth' 

experiment = f'{config.results_dir}/{config.method}'
experiment += f'-updatefreq={config.update_target_freq}'
experiment += f'-nupdates={config.n_model_updates}'
experiment += f'-split={config.split}'
experiment += f'-lrt={config.learning_rate}'
experiment += f'-nhidden={config.n_hidden}'
experiment += f'-rscale={config.rscale}'
experiment += f'-gamma={config.gamma}'
experiment += f'-posprob={config.pos_reward_prob}'
experiment += f'-gclip={config.grad_clip}'
experiment += f'-alpha={config.alpha}'
if 'ensemble-dqn' in config.method:
    experiment += f'-ensemble={config.n_ensemble}'
    
experiment += f'-seed={config.seed}/'
os.system(f'mkdir -p {experiment}')

print(f'will save as [{experiment}]')

agent = agents.Agent(config)
if config.cuda == 1: agent = agent.cuda()
    
replay_memory = torch.load(config.checkpoint_file)
print('[done]')
agent.replay_memory = replay_memory
rewards = []

if config.method == 'voronoi-truth':
    import gym
    env = gym.make('MountainCar-v0')
    a = search.VoronoiSearchTruth(env, max_steps=5000)
    
elif config.method == 'sil':
    agent.policy = models.Policy(config).cuda()
    optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate)
    print('warming up value function')
    for i in range(1):
        agent.train_policy_sil('train', 'explore', optimizer, config, alpha=config.alpha, beta=config.beta, n_updates=config.n_model_updates, nonzero_reward=False, warmup=True)
    
    print('training')
    for k in range(1000):
        
        for i in range(1):
            agent.train_policy_sil('train', 'explore', optimizer, config, alpha=config.alpha, beta=config.beta, n_updates=config.n_model_updates, nonzero_reward=True, warmup=False)
        log_string = f'step {k}, reward: ' + str(agent.act(env, config.split, config, policy='sil', n_episodes=100, eps_greedy=0)['reward'].item())
        print(log_string)
        
    
    
elif 'dqn' in config.method:
    eps_greedy = 0.0
    if config.method == 'dqn+random':
        agent.dqn = models.RandomFeatureDQN(config).cuda()
    elif config.method == 'dqn':
        agent.dqn = models.DQN(config).cuda()
    elif config.method == 'ensemble-dqn-mean' or config.method == 'ensemble-dqn-min':
        agent.dqn = models.EnsembleDQN(config).cuda()
    elif config.method == 'ensemble-dqn-mean-ll' or config.method == 'ensemble-dqn-min-ll':
        agent.dqn = models.EnsembleDQNLastLayer(config).cuda()
    optimizer = optim.Adam(agent.dqn.parameters(), lr=config.learning_rate)

    '''
    for k in range(20):
        loss, R = agent.pretrain_policy_dqn('train', 'explore', optimizer, config, n_updates=config.n_model_updates, reward_scale=config.rscale, grad_clip=config.grad_clip)
        log_string = f'pretraining, step {k} | Q loss: {loss:.8f}, R: {R:.4f}'
        print(log_string)
    '''
    
    for k in range(50000):
        plot_dir = f'{experiment}/value_plots/'
        os.system(f'mkdir -p {plot_dir}')
        loss, R = agent.train_policy_dqn('train', 'explore', optimizer, config, n_updates=config.n_model_updates, reward_scale=config.rscale, grad_clip=config.grad_clip, pos_reward_prob=config.pos_reward_prob)
        log_string = f'Q loss: {loss:.8f}, R: {R:.4f}'
        if config.method == 'dqn+model':
            loss_model, R_model = agent.train_policy_dqn_with_model('train', 'explore', optimizer, config, forward_model = agent.forward_model, n_updates=config.n_model_updates, reward_scale=config.rscale, grad_clip=config.grad_clip)
            log_string += f' | Q loss model: {loss_model:.8f}, R model: {R_model:.4f}'
        print(log_string)
        utils.logtxt(experiment + 'log.txt', log_string, date=True)
        if k % config.update_target_freq == 0:
            agent.dqn.sync_networks()
            utils.logtxt(experiment + 'log.txt', '[sync]', date=True)
        if k % 10 == 0:
            ep_reward = agent.act(env, config.split, config, policy='dqn', n_episodes=100, eps_greedy=eps_greedy)['reward'].item()
            log_string = f'step {k}, reward: ' + str(ep_reward)
            print(log_string)
            rewards.append(ep_reward)
            utils.logtxt(experiment + 'log.txt', log_string, date=True)
            torch.save(rewards, experiment + 'perf.pth')
            states, values = agent.test_policy_dqn('train', 'explore', optimizer, config, n_updates=100)
            states = torch.stack(states).cpu().view(-1, 2).numpy()
            values = torch.stack(values).cpu().detach().view(-1).numpy()
            plt.close()
            plt.scatter(states[:, 0], states[:, 1], c=values, s=2)
            plt.savefig(f'{plot_dir}/dqn_values_{k}.pdf')
            plt.close()
            logvalues = numpy.log(values - values.min() + 0.1)
            plt.scatter(states[:, 0], states[:, 1], c=logvalues, s=2)
            plt.savefig(f'{plot_dir}/dqn_log_values_{k}.pdf')
            plt.close()
            plt.hist(values, 200)
            plt.savefig(f'{plot_dir}/dqn_values_hist_{k}.pdf')
            plt.close()
            plt.hist(logvalues, 200)
            plt.savefig(f'{plot_dir}/dqn_log_values_hist_{k}.pdf')
            plt.close()


elif config.method == 'sac':
    eps_greedy = 0.0
    agent.sac = models.SAC(config).cuda()
    optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate)

    for k in range(3):
        print(k)
        agent.pretrain_policy_sac('train', 'explore', optimizer, config, alpha=config.alpha, beta=config.beta, n_updates=config.n_model_updates)
        
    for k in range(50000):
        plot_dir = f'{experiment}/value_plots/'
        os.system(f'mkdir -p {plot_dir}')
        q_loss, policy_loss, R, expected_q, entropy = agent.train_policy_sac('train', 'explore', optimizer, config, n_updates=config.n_model_updates, reward_scale=config.rscale, grad_clip=config.grad_clip, alpha=config.alpha)
        log_string = f'Q loss: {q_loss:.8f}, P loss: {policy_loss}, expected Q: {expected_q}, entropy: {entropy}, R: {R:.4f}'
        print(log_string)
        utils.logtxt(experiment + 'log.txt', log_string, date=True)
        if k % config.update_target_freq == 0:
            agent.sac.sync_networks()
            utils.logtxt(experiment + 'log.txt', '[sync]', date=True)
        if k % 10 == 0:
            log_string = f'step {k}, reward: ' + str(agent.act(env, config.split, config, policy='sac', n_episodes=100, eps_greedy=eps_greedy)['reward'].item())
            print(log_string)
            utils.logtxt(experiment + 'log.txt', log_string, date=True)
            states, values = agent.test_policy_sac('train', 'explore', optimizer, config, n_updates=100)
            states = torch.stack(states).cpu().view(-1, 2).numpy()
            values = torch.stack(values).cpu().detach().view(-1).numpy()
            plt.close()
            plt.scatter(states[:, 0], states[:, 1], c=values, s=2)
            plt.savefig(f'{plot_dir}/values_{k}.pdf')
            plt.close()
            logvalues = numpy.log(values - values.min() + 0.1)
            plt.scatter(states[:, 0], states[:, 1], c=logvalues, s=2)
            plt.savefig(f'{plot_dir}/log_values_{k}.pdf')
            plt.close()
            plt.hist(values, 200)
            plt.savefig(f'{plot_dir}/values_hist_{k}.pdf')
            plt.close()
            plt.hist(logvalues, 200)
            plt.savefig(f'{plot_dir}/log_values_hist_{k}.pdf')
            plt.close()
            
            
            
            


